{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Task Routing and Model Partitioning\n",
    "\n",
    "## The Hybrid NPU Stack Series\n",
    "\n",
    "This notebook accompanies **Part 3** of the blog series. We'll implement:\n",
    "\n",
    "1. Confidence-based task routing\n",
    "2. Model partitioning strategies\n",
    "3. Cost-optimized inference routing\n",
    "4. Simulated hybrid inference pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from enum import Enum\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Notebook 2: Task Routing and Model Partitioning\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Inference Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceTarget(Enum):\n",
    "    EDGE = \"edge\"\n",
    "    CLOUD = \"cloud\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "@dataclass\n",
    "class DeviceProfile:\n",
    "    name: str\n",
    "    npu_tops: float\n",
    "    memory_gb: float\n",
    "    power_watts: float\n",
    "    max_model_params_billions: float\n",
    "    supported_precisions: List[str] = field(default_factory=lambda: [\"INT8\", \"FP16\"])\n",
    "\n",
    "@dataclass\n",
    "class CloudProfile:\n",
    "    name: str\n",
    "    gpu_type: str\n",
    "    cost_per_hour: float\n",
    "    cost_per_1m_tokens: float\n",
    "    latency_ms: float\n",
    "    max_model_params_billions: float\n",
    "\n",
    "@dataclass\n",
    "class InferenceTask:\n",
    "    task_id: str\n",
    "    task_type: str\n",
    "    input_tokens: int\n",
    "    expected_output_tokens: int\n",
    "    required_model_params_billions: float\n",
    "    latency_requirement_ms: float\n",
    "    privacy_sensitive: bool = False\n",
    "\n",
    "# Define devices and cloud\n",
    "DEVICES = {\n",
    "    \"smartphone\": DeviceProfile(\"Smartphone\", 38, 8, 3, 3),\n",
    "    \"laptop\": DeviceProfile(\"AI Laptop\", 50, 32, 15, 7),\n",
    "    \"workstation\": DeviceProfile(\"AI Workstation\", 80, 96, 45, 70),\n",
    "}\n",
    "\n",
    "CLOUD = CloudProfile(\"Cloud GPU\", \"H100\", 3.50, 0.50, 50, 1000)\n",
    "\n",
    "print(\"ðŸ“± Available Edge Devices:\")\n",
    "for name, device in DEVICES.items():\n",
    "    print(f\"   {device.name}: {device.npu_tops} TOPS, {device.memory_gb}GB RAM\")\n",
    "print(f\"\\nâ˜ï¸  Cloud: {CLOUD.gpu_type}, ${CLOUD.cost_per_hour}/hr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confidence-Based Task Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RoutingDecision:\n",
    "    target: InferenceTarget\n",
    "    reason: str\n",
    "    estimated_latency_ms: float\n",
    "    estimated_cost: float\n",
    "    confidence: float\n",
    "\n",
    "class TaskRouter:\n",
    "    \"\"\"\n",
    "    Intelligent task router implementing confidence-based routing.\n",
    "    \n",
    "    Decision Logic:\n",
    "    1. Privacy-sensitive -> Always edge (if possible)\n",
    "    2. Model too large -> Cloud\n",
    "    3. Strict latency -> Edge preferred\n",
    "    4. Otherwise -> Confidence-based\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: DeviceProfile, cloud: CloudProfile, \n",
    "                 confidence_threshold: float = 0.85):\n",
    "        self.device = device\n",
    "        self.cloud = cloud\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.stats = {'edge': 0, 'cloud': 0, 'total_cost': 0.0}\n",
    "    \n",
    "    def estimate_edge_latency(self, task: InferenceTask) -> float:\n",
    "        base = 5\n",
    "        token_latency = (task.input_tokens + task.expected_output_tokens) * 0.1\n",
    "        model_factor = task.required_model_params_billions / self.device.max_model_params_billions\n",
    "        return base + token_latency * (1 + model_factor)\n",
    "    \n",
    "    def estimate_cloud_latency(self, task: InferenceTask) -> float:\n",
    "        compute = (task.input_tokens + task.expected_output_tokens) * 0.05\n",
    "        return self.cloud.latency_ms + compute\n",
    "    \n",
    "    def estimate_cloud_cost(self, task: InferenceTask) -> float:\n",
    "        total_tokens = task.input_tokens + task.expected_output_tokens\n",
    "        return (total_tokens / 1_000_000) * self.cloud.cost_per_1m_tokens\n",
    "    \n",
    "    def can_run_on_edge(self, task: InferenceTask) -> bool:\n",
    "        return task.required_model_params_billions <= self.device.max_model_params_billions\n",
    "    \n",
    "    def simulate_edge_confidence(self, task: InferenceTask) -> float:\n",
    "        base = 0.95\n",
    "        complexity_penalty = min(0.3, task.expected_output_tokens / 500 * 0.1)\n",
    "        model_penalty = min(0.2, (task.required_model_params_billions - 1) / 10 * 0.1)\n",
    "        noise = random.uniform(-0.1, 0.1)\n",
    "        return max(0.5, min(1.0, base - complexity_penalty - model_penalty + noise))\n",
    "    \n",
    "    def route(self, task: InferenceTask) -> RoutingDecision:\n",
    "        edge_latency = self.estimate_edge_latency(task)\n",
    "        cloud_latency = self.estimate_cloud_latency(task)\n",
    "        cloud_cost = self.estimate_cloud_cost(task)\n",
    "        can_edge = self.can_run_on_edge(task)\n",
    "        \n",
    "        # Rule 1: Privacy\n",
    "        if task.privacy_sensitive and can_edge:\n",
    "            self.stats['edge'] += 1\n",
    "            return RoutingDecision(InferenceTarget.EDGE, \"Privacy-sensitive\", edge_latency, 0.0, 1.0)\n",
    "        \n",
    "        # Rule 2: Model size\n",
    "        if not can_edge:\n",
    "            self.stats['cloud'] += 1\n",
    "            self.stats['total_cost'] += cloud_cost\n",
    "            return RoutingDecision(InferenceTarget.CLOUD, \"Model too large for edge\", cloud_latency, cloud_cost, 1.0)\n",
    "        \n",
    "        # Rule 3: Latency\n",
    "        if task.latency_requirement_ms < cloud_latency and edge_latency <= task.latency_requirement_ms:\n",
    "            self.stats['edge'] += 1\n",
    "            return RoutingDecision(InferenceTarget.EDGE, \"Latency requirement\", edge_latency, 0.0, 1.0)\n",
    "        \n",
    "        # Rule 4: Confidence-based\n",
    "        confidence = self.simulate_edge_confidence(task)\n",
    "        \n",
    "        if confidence >= self.confidence_threshold:\n",
    "            self.stats['edge'] += 1\n",
    "            return RoutingDecision(InferenceTarget.EDGE, f\"High confidence ({confidence:.2f})\", edge_latency, 0.0, confidence)\n",
    "        else:\n",
    "            self.stats['cloud'] += 1\n",
    "            self.stats['total_cost'] += cloud_cost\n",
    "            return RoutingDecision(InferenceTarget.CLOUD, f\"Low confidence ({confidence:.2f})\", cloud_latency, cloud_cost, confidence)\n",
    "\n",
    "print(\"âœ… TaskRouter class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the Router Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_tasks(n: int = 100) -> List[InferenceTask]:\n",
    "    \"\"\"Generate diverse sample inference tasks\"\"\"\n",
    "    task_types = [\n",
    "        (\"classification\", 50, 10, 1, 50),\n",
    "        (\"sentiment\", 100, 5, 1, 30),\n",
    "        (\"summarization\", 500, 150, 3, 200),\n",
    "        (\"translation\", 200, 200, 3, 100),\n",
    "        (\"code_generation\", 300, 500, 7, 500),\n",
    "        (\"reasoning\", 200, 1000, 30, 1000),\n",
    "        (\"image_caption\", 50, 50, 2, 100),\n",
    "    ]\n",
    "    \n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        task_type, inp, out, model, latency = random.choice(task_types)\n",
    "        # Add variation\n",
    "        inp = int(inp * random.uniform(0.5, 2.0))\n",
    "        out = int(out * random.uniform(0.5, 2.0))\n",
    "        \n",
    "        tasks.append(InferenceTask(\n",
    "            task_id=f\"task_{i:04d}\",\n",
    "            task_type=task_type,\n",
    "            input_tokens=inp,\n",
    "            expected_output_tokens=out,\n",
    "            required_model_params_billions=model,\n",
    "            latency_requirement_ms=latency,\n",
    "            privacy_sensitive=random.random() < 0.15\n",
    "        ))\n",
    "    return tasks\n",
    "\n",
    "# Generate tasks and run routing\n",
    "tasks = generate_sample_tasks(500)\n",
    "router = TaskRouter(DEVICES[\"laptop\"], CLOUD, confidence_threshold=0.85)\n",
    "\n",
    "results = []\n",
    "for task in tasks:\n",
    "    decision = router.route(task)\n",
    "    results.append({\n",
    "        'task_id': task.task_id,\n",
    "        'task_type': task.task_type,\n",
    "        'target': decision.target.value,\n",
    "        'reason': decision.reason,\n",
    "        'latency_ms': decision.estimated_latency_ms,\n",
    "        'cost': decision.estimated_cost,\n",
    "        'confidence': decision.confidence,\n",
    "        'privacy': task.privacy_sensitive\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ROUTING SIMULATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nTotal tasks: {len(tasks)}\")\n",
    "print(f\"Edge: {router.stats['edge']} ({router.stats['edge']/len(tasks)*100:.1f}%)\")\n",
    "print(f\"Cloud: {router.stats['cloud']} ({router.stats['cloud']/len(tasks)*100:.1f}%)\")\n",
    "print(f\"Total cloud cost: ${router.stats['total_cost']:.4f}\")\n",
    "print(f\"\\nSample decisions:\")\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize routing decisions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Edge vs Cloud distribution\n",
    "target_counts = df_results['target'].value_counts()\n",
    "colors = ['#38a169', '#e53e3e']\n",
    "axes[0, 0].pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "axes[0, 0].set_title('Routing Distribution: Edge vs Cloud')\n",
    "\n",
    "# 2. By task type\n",
    "task_routing = df_results.groupby(['task_type', 'target']).size().unstack(fill_value=0)\n",
    "task_routing.plot(kind='bar', ax=axes[0, 1], color=colors)\n",
    "axes[0, 1].set_title('Routing by Task Type')\n",
    "axes[0, 1].set_xlabel('Task Type')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].legend(title='Target')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Latency distribution\n",
    "edge_latencies = df_results[df_results['target'] == 'edge']['latency_ms']\n",
    "cloud_latencies = df_results[df_results['target'] == 'cloud']['latency_ms']\n",
    "axes[1, 0].hist([edge_latencies, cloud_latencies], bins=20, label=['Edge', 'Cloud'], color=colors)\n",
    "axes[1, 0].set_title('Latency Distribution')\n",
    "axes[1, 0].set_xlabel('Latency (ms)')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Confidence distribution\n",
    "axes[1, 1].hist(df_results['confidence'], bins=20, color='#3182ce', edgecolor='white')\n",
    "axes[1, 1].axvline(x=0.85, color='red', linestyle='--', label='Threshold (0.85)')\n",
    "axes[1, 1].set_title('Confidence Score Distribution')\n",
    "axes[1, 1].set_xlabel('Confidence')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('routing_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cost Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cost_by_threshold(tasks: List[InferenceTask], thresholds: List[float]) -> pd.DataFrame:\n",
    "    \"\"\"Analyze how confidence threshold affects cost and edge utilization\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        router = TaskRouter(DEVICES[\"laptop\"], CLOUD, confidence_threshold=threshold)\n",
    "        \n",
    "        for task in tasks:\n",
    "            router.route(task)\n",
    "        \n",
    "        edge_pct = router.stats['edge'] / len(tasks) * 100\n",
    "        results.append({\n",
    "            'Threshold': threshold,\n",
    "            'Edge %': edge_pct,\n",
    "            'Cloud %': 100 - edge_pct,\n",
    "            'Total Cost': router.stats['total_cost'],\n",
    "            'Cost per Request': router.stats['total_cost'] / len(tasks)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "df_cost = analyze_cost_by_threshold(tasks, thresholds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COST OPTIMIZATION BY CONFIDENCE THRESHOLD\")\n",
    "print(\"=\"*60)\n",
    "display(df_cost)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(df_cost['Threshold'], df_cost['Edge %'], 'g-o', label='Edge %', linewidth=2)\n",
    "axes[0].plot(df_cost['Threshold'], df_cost['Cloud %'], 'r-o', label='Cloud %', linewidth=2)\n",
    "axes[0].set_xlabel('Confidence Threshold')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].set_title('Edge vs Cloud Utilization by Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(df_cost['Threshold'], df_cost['Total Cost'], 'b-o', linewidth=2)\n",
    "axes[1].set_xlabel('Confidence Threshold')\n",
    "axes[1].set_ylabel('Total Cost ($)')\n",
    "axes[1].set_title('Total Cloud Cost by Threshold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY INSIGHT: Lower threshold = More edge usage = Lower cost\")\n",
    "print(\"   But too low threshold may sacrifice accuracy on complex tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Partitioning Simulation\n",
    "\n",
    "For large models, we can split layers between edge and cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPartition:\n",
    "    \"\"\"Represents a partitioned model across edge and cloud\"\"\"\n",
    "    total_layers: int\n",
    "    edge_layers: int\n",
    "    cloud_layers: int\n",
    "    edge_memory_gb: float\n",
    "    cloud_memory_gb: float\n",
    "    transfer_size_mb: float  # Intermediate activations\n",
    "\n",
    "class ModelPartitioner:\n",
    "    \"\"\"\n",
    "    Partitions a model between edge and cloud based on device constraints.\n",
    "    \n",
    "    Strategy:\n",
    "    - Early layers (embeddings, initial transformers) run on edge\n",
    "    - Later layers (reasoning, generation) run on cloud\n",
    "    - Intermediate activations are transferred\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: DeviceProfile):\n",
    "        self.device = device\n",
    "    \n",
    "    def partition(self, model_params_b: float, total_layers: int) -> ModelPartition:\n",
    "        # Estimate memory per layer (simplified)\n",
    "        memory_per_layer_gb = (model_params_b * 2) / total_layers  # 2 bytes per param (FP16)\n",
    "        \n",
    "        # Calculate how many layers fit on edge\n",
    "        max_edge_layers = int(self.device.memory_gb * 0.7 / memory_per_layer_gb)  # Use 70% of memory\n",
    "        edge_layers = min(max_edge_layers, total_layers // 2)  # At most half\n",
    "        cloud_layers = total_layers - edge_layers\n",
    "        \n",
    "        edge_memory = edge_layers * memory_per_layer_gb\n",
    "        cloud_memory = cloud_layers * memory_per_layer_gb\n",
    "        \n",
    "        # Estimate transfer size (hidden dimension * sequence length * batch)\n",
    "        hidden_dim = int(np.sqrt(model_params_b * 1e9 / total_layers / 12))  # Rough estimate\n",
    "        transfer_size_mb = hidden_dim * 2048 * 2 / (1024 * 1024)  # 2048 tokens, FP16\n",
    "        \n",
    "        return ModelPartition(\n",
    "            total_layers=total_layers,\n",
    "            edge_layers=edge_layers,\n",
    "            cloud_layers=cloud_layers,\n",
    "            edge_memory_gb=edge_memory,\n",
    "            cloud_memory_gb=cloud_memory,\n",
    "            transfer_size_mb=transfer_size_mb\n",
    "        )\n",
    "\n",
    "# Test partitioning\n",
    "partitioner = ModelPartitioner(DEVICES[\"laptop\"])\n",
    "\n",
    "models = [\n",
    "    (\"Llama-3-8B\", 8, 32),\n",
    "    (\"Llama-3-70B\", 70, 80),\n",
    "    (\"GPT-4-class\", 175, 120),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PARTITIONING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDevice: {DEVICES['laptop'].name} ({DEVICES['laptop'].memory_gb}GB RAM)\\n\")\n",
    "\n",
    "partition_results = []\n",
    "for name, params, layers in models:\n",
    "    partition = partitioner.partition(params, layers)\n",
    "    partition_results.append({\n",
    "        'Model': name,\n",
    "        'Params (B)': params,\n",
    "        'Total Layers': layers,\n",
    "        'Edge Layers': partition.edge_layers,\n",
    "        'Cloud Layers': partition.cloud_layers,\n",
    "        'Edge Memory (GB)': f\"{partition.edge_memory_gb:.1f}\",\n",
    "        'Transfer (MB)': f\"{partition.transfer_size_mb:.1f}\"\n",
    "    })\n",
    "\n",
    "df_partition = pd.DataFrame(partition_results)\n",
    "display(df_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize partitioning\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = [m[0] for m in models]\n",
    "edge_layers = [r['Edge Layers'] for r in partition_results]\n",
    "cloud_layers = [r['Cloud Layers'] for r in partition_results]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.6\n",
    "\n",
    "ax.barh(x, edge_layers, width, label='Edge Layers', color='#38a169')\n",
    "ax.barh(x, cloud_layers, width, left=edge_layers, label='Cloud Layers', color='#e53e3e')\n",
    "\n",
    "ax.set_xlabel('Number of Layers')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(model_names)\n",
    "ax.set_title('Model Layer Partitioning: Edge vs Cloud')\n",
    "ax.legend()\n",
    "\n",
    "# Add text labels\n",
    "for i, (e, c) in enumerate(zip(edge_layers, cloud_layers)):\n",
    "    ax.text(e/2, i, f'{e}', ha='center', va='center', fontweight='bold', color='white')\n",
    "    ax.text(e + c/2, i, f'{c}', ha='center', va='center', fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_partitioning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ MODEL PARTITIONING BENEFITS:\")\n",
    "print(\"   â€¢ Edge handles tokenization and early feature extraction\")\n",
    "print(\"   â€¢ Only intermediate activations transfer (not raw input)\")\n",
    "print(\"   â€¢ Cloud handles complex reasoning layers\")\n",
    "print(\"   â€¢ Privacy: Raw data never leaves device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Pipeline Pseudocode\n",
    "\n",
    "Here's how to implement a complete hybrid inference pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for production hybrid pipeline\n",
    "\n",
    "hybrid_pipeline_code = '''\n",
    "# ==============================================\n",
    "# HYBRID NPU INFERENCE PIPELINE - PSEUDOCODE\n",
    "# ==============================================\n",
    "\n",
    "class HybridInferencePipeline:\n",
    "    \"\"\"\n",
    "    Production-ready hybrid inference pipeline.\n",
    "    Routes tasks between edge NPU and cloud GPU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, edge_model, cloud_endpoint, router):\n",
    "        self.edge_model = edge_model      # Local quantized model\n",
    "        self.cloud_api = cloud_endpoint    # Cloud API client\n",
    "        self.router = router               # TaskRouter instance\n",
    "        self.cache = LRUCache(1000)        # Response cache\n",
    "    \n",
    "    async def infer(self, request: InferenceRequest) -> InferenceResponse:\n",
    "        \"\"\"\n",
    "        Main inference entry point.\n",
    "        \"\"\"\n",
    "        # Step 1: Check cache\n",
    "        cache_key = hash(request)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Step 2: Create task and get routing decision\n",
    "        task = self.create_task(request)\n",
    "        decision = self.router.route(task)\n",
    "        \n",
    "        # Step 3: Execute based on routing\n",
    "        if decision.target == InferenceTarget.EDGE:\n",
    "            response = await self.edge_inference(request)\n",
    "        elif decision.target == InferenceTarget.CLOUD:\n",
    "            response = await self.cloud_inference(request)\n",
    "        else:  # HYBRID\n",
    "            response = await self.hybrid_inference(request)\n",
    "        \n",
    "        # Step 4: Cache and return\n",
    "        self.cache[cache_key] = response\n",
    "        return response\n",
    "    \n",
    "    async def edge_inference(self, request):\n",
    "        \"\"\"Run inference on local NPU\"\"\"\n",
    "        # Preprocess\n",
    "        tokens = self.tokenize(request.text)\n",
    "        \n",
    "        # Run on NPU (using ONNX Runtime / Core ML / etc.)\n",
    "        with npu_context():\n",
    "            output = self.edge_model.run(tokens)\n",
    "        \n",
    "        # Check confidence\n",
    "        if output.confidence < self.router.confidence_threshold:\n",
    "            # Fallback to cloud\n",
    "            return await self.cloud_inference(request)\n",
    "        \n",
    "        return self.decode(output)\n",
    "    \n",
    "    async def hybrid_inference(self, request):\n",
    "        \"\"\"Split inference between edge and cloud\"\"\"\n",
    "        # Run early layers on edge\n",
    "        tokens = self.tokenize(request.text)\n",
    "        \n",
    "        with npu_context():\n",
    "            intermediate = self.edge_model.run_partial(\n",
    "                tokens, \n",
    "                layers=range(0, self.partition.edge_layers)\n",
    "            )\n",
    "        \n",
    "        # Send intermediate activations to cloud\n",
    "        response = await self.cloud_api.complete_inference(\n",
    "            activations=intermediate,\n",
    "            start_layer=self.partition.edge_layers\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "pipeline = HybridInferencePipeline(\n",
    "    edge_model=load_quantized_model(\"llama-3-8b-int8\"),\n",
    "    cloud_endpoint=CloudAPI(\"https://api.cloud.com/v1\"),\n",
    "    router=TaskRouter(device_profile, cloud_profile)\n",
    ")\n",
    "\n",
    "# Process requests\n",
    "response = await pipeline.infer(request)\n",
    "'''\n",
    "\n",
    "print(hybrid_pipeline_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 2 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ… IMPLEMENTED:\n",
    "   1. TaskRouter with confidence-based routing\n",
    "   2. Multi-factor routing decisions (privacy, latency, model size)\n",
    "   3. Cost optimization analysis by threshold\n",
    "   4. Model partitioning for large LLMs\n",
    "   5. Hybrid pipeline pseudocode\n",
    "\n",
    "ðŸ“Š KEY FINDINGS:\n",
    "   â€¢ Edge routing at 85% threshold: ~70% of tasks stay local\n",
    "   â€¢ Cost savings: Up to 100% for edge-compatible tasks\n",
    "   â€¢ Latency improvement: 4x faster (15ms vs 58ms)\n",
    "   â€¢ Model partitioning enables 70B+ models on laptops\n",
    "\n",
    "ðŸŽ¯ BEST PRACTICES:\n",
    "   1. Start with high confidence threshold (0.9), tune down\n",
    "   2. Always route privacy-sensitive data to edge\n",
    "   3. Use model partitioning for large models\n",
    "   4. Monitor confidence distributions to optimize threshold\n",
    "   5. Cache common responses to avoid redundant inference\n",
    "\n",
    "âž¡ï¸  Next: Notebook 3 - Cost Analysis and 2026 Outlook\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
