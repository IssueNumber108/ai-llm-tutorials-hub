{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: NPU Fundamentals and Benchmarking\n",
    "\n",
    "## The Hybrid NPU Stack Series\n",
    "\n",
    "This notebook accompanies **Part 1 & 2** of the blog series on Hybrid NPU Stacks. We'll explore:\n",
    "\n",
    "1. Understanding processor differences (CPU vs GPU vs NPU)\n",
    "2. Benchmarking inference performance\n",
    "3. Measuring power efficiency\n",
    "4. Detecting available hardware accelerators\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary packages. We'll use lightweight libraries that work across different hardware configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment and run if needed\n",
    "# !pip install numpy pandas matplotlib psutil onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import psutil\n",
    "import platform\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Platform: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Processor Architectures\n",
    "\n",
    "### Key Differences: CPU vs GPU vs NPU vs TPU\n",
    "\n",
    "Let's create a visual comparison of different processor types and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessorProfile:\n",
    "    \"\"\"Profile of a processor type for AI workloads\"\"\"\n",
    "    name: str\n",
    "    best_for: str\n",
    "    power_watts_min: float\n",
    "    power_watts_max: float\n",
    "    tops_min: float\n",
    "    tops_max: float\n",
    "    typical_use_case: str\n",
    "    parallelism: str\n",
    "    precision: str\n",
    "\n",
    "# Define processor profiles based on 2025 industry data\n",
    "processors = [\n",
    "    ProcessorProfile(\n",
    "        name=\"CPU\",\n",
    "        best_for=\"Sequential tasks, control logic\",\n",
    "        power_watts_min=15,\n",
    "        power_watts_max=150,\n",
    "        tops_min=0.1,\n",
    "        tops_max=1.0,\n",
    "        typical_use_case=\"General compute, orchestration\",\n",
    "        parallelism=\"Low (4-64 cores)\",\n",
    "        precision=\"FP32, FP64\"\n",
    "    ),\n",
    "    ProcessorProfile(\n",
    "        name=\"GPU\",\n",
    "        best_for=\"Parallel workloads, training\",\n",
    "        power_watts_min=75,\n",
    "        power_watts_max=700,\n",
    "        tops_min=100,\n",
    "        tops_max=3000,\n",
    "        typical_use_case=\"Training, complex inference\",\n",
    "        parallelism=\"Very High (thousands of cores)\",\n",
    "        precision=\"FP32, FP16, INT8\"\n",
    "    ),\n",
    "    ProcessorProfile(\n",
    "        name=\"NPU\",\n",
    "        best_for=\"Inference, always-on AI\",\n",
    "        power_watts_min=1,\n",
    "        power_watts_max=15,\n",
    "        tops_min=10,\n",
    "        tops_max=80,\n",
    "        typical_use_case=\"Edge inference, mobile AI\",\n",
    "        parallelism=\"Medium (optimized matrix ops)\",\n",
    "        precision=\"INT8, INT4, FP16\"\n",
    "    ),\n",
    "    ProcessorProfile(\n",
    "        name=\"TPU\",\n",
    "        best_for=\"Large-scale ML\",\n",
    "        power_watts_min=200,\n",
    "        power_watts_max=450,\n",
    "        tops_min=200,\n",
    "        tops_max=400,\n",
    "        typical_use_case=\"Cloud training/inference\",\n",
    "        parallelism=\"High (systolic arrays)\",\n",
    "        precision=\"BF16, INT8\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_processors = pd.DataFrame([\n",
    "    {\n",
    "        'Processor': p.name,\n",
    "        'Best For': p.best_for,\n",
    "        'Power (W)': f\"{p.power_watts_min}-{p.power_watts_max}\",\n",
    "        'TOPS Range': f\"{p.tops_min}-{p.tops_max}\",\n",
    "        'Use Case': p.typical_use_case,\n",
    "        'Parallelism': p.parallelism,\n",
    "        'Precision': p.precision\n",
    "    }\n",
    "    for p in processors\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSOR COMPARISON MATRIX\")\n",
    "print(\"=\"*80)\n",
    "display(df_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TOPS per Watt efficiency\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Power consumption range\n",
    "colors = ['#3182ce', '#e53e3e', '#38a169', '#805ad5']\n",
    "names = [p.name for p in processors]\n",
    "power_min = [p.power_watts_min for p in processors]\n",
    "power_max = [p.power_watts_max for p in processors]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "axes[0].bar(x, power_max, color=colors, alpha=0.3, label='Max')\n",
    "axes[0].bar(x, power_min, color=colors, label='Min')\n",
    "axes[0].set_xlabel('Processor Type')\n",
    "axes[0].set_ylabel('Power Consumption (Watts)')\n",
    "axes[0].set_title('Power Consumption Range')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(names)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot 2: TOPS range\n",
    "tops_min = [p.tops_min for p in processors]\n",
    "tops_max = [p.tops_max for p in processors]\n",
    "\n",
    "axes[1].bar(x, tops_max, color=colors, alpha=0.3, label='Max')\n",
    "axes[1].bar(x, tops_min, color=colors, label='Min')\n",
    "axes[1].set_xlabel('Processor Type')\n",
    "axes[1].set_ylabel('TOPS (Trillion Operations/Second)')\n",
    "axes[1].set_title('Compute Performance Range')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(names)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Plot 3: TOPS per Watt (efficiency)\n",
    "efficiency = []\n",
    "for p in processors:\n",
    "    avg_tops = (p.tops_min + p.tops_max) / 2\n",
    "    avg_power = (p.power_watts_min + p.power_watts_max) / 2\n",
    "    efficiency.append(avg_tops / avg_power)\n",
    "\n",
    "axes[2].bar(x, efficiency, color=colors)\n",
    "axes[2].set_xlabel('Processor Type')\n",
    "axes[2].set_ylabel('TOPS per Watt')\n",
    "axes[2].set_title('Energy Efficiency (Higher is Better)')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('processor_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š KEY INSIGHT: NPUs offer the best TOPS-per-Watt efficiency,\")\n",
    "print(\"   making them ideal for battery-powered edge devices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detecting Hardware Accelerators\n",
    "\n",
    "Let's build a utility to detect what AI acceleration hardware is available on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hardware_accelerators() -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Detect available hardware accelerators for AI inference.\n",
    "    Returns a dictionary with detected hardware and capabilities.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cpu': {},\n",
    "        'gpu': {},\n",
    "        'npu': {},\n",
    "        'onnx_providers': []\n",
    "    }\n",
    "    \n",
    "    # CPU Detection\n",
    "    results['cpu'] = {\n",
    "        'processor': platform.processor(),\n",
    "        'cores_physical': psutil.cpu_count(logical=False),\n",
    "        'cores_logical': psutil.cpu_count(logical=True),\n",
    "        'architecture': platform.machine(),\n",
    "        'available': True\n",
    "    }\n",
    "    \n",
    "    # GPU Detection (PyTorch)\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            results['gpu'] = {\n",
    "                'available': True,\n",
    "                'type': 'CUDA',\n",
    "                'device_count': torch.cuda.device_count(),\n",
    "                'device_name': torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else 'N/A'\n",
    "            }\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            results['gpu'] = {\n",
    "                'available': True,\n",
    "                'type': 'Apple MPS (Metal)',\n",
    "                'device_count': 1,\n",
    "                'device_name': 'Apple Silicon GPU'\n",
    "            }\n",
    "        else:\n",
    "            results['gpu']['available'] = False\n",
    "    except ImportError:\n",
    "        results['gpu']['available'] = False\n",
    "        results['gpu']['note'] = 'PyTorch not installed'\n",
    "    \n",
    "    # ONNX Runtime Provider Detection\n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        results['onnx_providers'] = ort.get_available_providers()\n",
    "        \n",
    "        # Check for NPU-related providers\n",
    "        npu_providers = ['DmlExecutionProvider', 'QNNExecutionProvider', \n",
    "                        'CoreMLExecutionProvider', 'OpenVINOExecutionProvider']\n",
    "        detected_npu = [p for p in results['onnx_providers'] if p in npu_providers]\n",
    "        \n",
    "        if detected_npu:\n",
    "            results['npu'] = {\n",
    "                'available': True,\n",
    "                'providers': detected_npu\n",
    "            }\n",
    "        else:\n",
    "            results['npu']['available'] = False\n",
    "            \n",
    "    except ImportError:\n",
    "        results['onnx_providers'] = ['Not installed']\n",
    "        results['npu']['available'] = False\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run detection\n",
    "hardware = detect_hardware_accelerators()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HARDWARE ACCELERATOR DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ–¥ï¸  CPU:\")\n",
    "for key, value in hardware['cpu'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸŽ® GPU:\")\n",
    "for key, value in hardware['gpu'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ§  NPU:\")\n",
    "for key, value in hardware['npu'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ“¦ ONNX Runtime Providers:\")\n",
    "for provider in hardware['onnx_providers']:\n",
    "    print(f\"   - {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking Neural Network Operations\n",
    "\n",
    "Let's benchmark the core operations that NPUs optimize for: matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(sizes: List[int], iterations: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark matrix multiplication at different sizes.\n",
    "    This is the core operation in neural network inference.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Create random matrices (simulating weights and activations)\n",
    "        A = np.random.randn(size, size).astype(np.float32)\n",
    "        B = np.random.randn(size, size).astype(np.float32)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = np.matmul(A, B)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(iterations):\n",
    "            start = time.perf_counter()\n",
    "            _ = np.matmul(A, B)\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "        std_time = np.std(times) * 1000\n",
    "        \n",
    "        # Calculate GFLOPS (2 * N^3 for matrix multiply)\n",
    "        flops = 2 * (size ** 3)\n",
    "        gflops = (flops / (avg_time / 1000)) / 1e9  # GFLOPS\n",
    "        \n",
    "        results.append({\n",
    "            'Matrix Size': f\"{size}x{size}\",\n",
    "            'Time (ms)': f\"{avg_time:.3f} Â± {std_time:.3f}\",\n",
    "            'GFLOPS': f\"{gflops:.2f}\",\n",
    "            'Operations': f\"{flops/1e9:.2f}B\"\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATRIX MULTIPLICATION BENCHMARK (CPU/NumPy)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis simulates the core operation in neural network layers.\")\n",
    "print(\"NPUs optimize specifically for this type of computation.\\n\")\n",
    "\n",
    "sizes = [128, 256, 512, 1024, 2048]\n",
    "df_benchmark = benchmark_matmul(sizes)\n",
    "display(df_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_precision_comparison(size: int = 1024, iterations: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare performance across different numeric precisions.\n",
    "    NPUs excel at lower precision (INT8, FP16) operations.\n",
    "    \"\"\"\n",
    "    precisions = [\n",
    "        ('float64', np.float64, 'FP64 - Full precision'),\n",
    "        ('float32', np.float32, 'FP32 - Standard'),\n",
    "        ('float16', np.float16, 'FP16 - Half precision (NPU optimized)'),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    base_time = None\n",
    "    \n",
    "    for name, dtype, description in precisions:\n",
    "        A = np.random.randn(size, size).astype(dtype)\n",
    "        B = np.random.randn(size, size).astype(dtype)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = np.matmul(A, B)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(iterations):\n",
    "            start = time.perf_counter()\n",
    "            _ = np.matmul(A, B)\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        avg_time = np.mean(times) * 1000\n",
    "        memory_mb = (A.nbytes + B.nbytes) / (1024 * 1024)\n",
    "        \n",
    "        if base_time is None:\n",
    "            base_time = avg_time\n",
    "        \n",
    "        results.append({\n",
    "            'Precision': name,\n",
    "            'Description': description,\n",
    "            'Time (ms)': f\"{avg_time:.3f}\",\n",
    "            'Memory (MB)': f\"{memory_mb:.1f}\",\n",
    "            'Relative Speed': f\"{base_time / avg_time:.2f}x\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRECISION COMPARISON BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMatrix size: 1024x1024\")\n",
    "print(\"NPUs are optimized for lower precision (FP16, INT8) operations.\\n\")\n",
    "\n",
    "df_precision = benchmark_precision_comparison()\n",
    "display(df_precision)\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY INSIGHT: Lower precision = faster inference + less memory.\")\n",
    "print(\"   This is why NPU quantization (INT8, INT4) is so powerful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edge vs Cloud Inference Simulation\n",
    "\n",
    "Let's model what happens when you split inference between edge NPU and cloud GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    \"\"\"Configuration for inference simulation\"\"\"\n",
    "    name: str\n",
    "    compute_time_ms: float\n",
    "    network_latency_ms: float\n",
    "    cost_per_inference: float\n",
    "    power_watts: float\n",
    "\n",
    "# Define inference configurations\n",
    "edge_npu = InferenceConfig(\n",
    "    name=\"Edge NPU\",\n",
    "    compute_time_ms=15,\n",
    "    network_latency_ms=0,\n",
    "    cost_per_inference=0.0,\n",
    "    power_watts=5\n",
    ")\n",
    "\n",
    "cloud_gpu = InferenceConfig(\n",
    "    name=\"Cloud GPU\",\n",
    "    compute_time_ms=8,\n",
    "    network_latency_ms=50,\n",
    "    cost_per_inference=0.00005,\n",
    "    power_watts=300\n",
    ")\n",
    "\n",
    "def simulate_inference_batch(config: InferenceConfig, batch_size: int = 1000) -> Dict:\n",
    "    \"\"\"Simulate a batch of inference requests.\"\"\"\n",
    "    total_time_ms = batch_size * (config.compute_time_ms + config.network_latency_ms)\n",
    "    total_cost = batch_size * config.cost_per_inference\n",
    "    \n",
    "    return {\n",
    "        'config': config.name,\n",
    "        'batch_size': batch_size,\n",
    "        'total_time_seconds': total_time_ms / 1000,\n",
    "        'avg_latency_ms': config.compute_time_ms + config.network_latency_ms,\n",
    "        'total_cost': total_cost,\n",
    "        'throughput_per_sec': batch_size / (total_time_ms / 1000)\n",
    "    }\n",
    "\n",
    "# Run simulations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EDGE vs CLOUD INFERENCE SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_sizes = [1000, 10000, 100000, 1000000]\n",
    "\n",
    "results_edge = [simulate_inference_batch(edge_npu, batch) for batch in batch_sizes]\n",
    "results_cloud = [simulate_inference_batch(cloud_gpu, batch) for batch in batch_sizes]\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for e, c in zip(results_edge, results_cloud):\n",
    "    savings = ((c['total_cost'] - e['total_cost']) / max(c['total_cost'], 0.01)) * 100\n",
    "    comparison_data.append({\n",
    "        'Batch Size': f\"{e['batch_size']:,}\",\n",
    "        'Edge Latency (ms)': e['avg_latency_ms'],\n",
    "        'Cloud Latency (ms)': c['avg_latency_ms'],\n",
    "        'Edge Cost': f\"${e['total_cost']:.2f}\",\n",
    "        'Cloud Cost': f\"${c['total_cost']:.2f}\",\n",
    "        'Cost Savings': f\"{savings:.0f}%\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cost comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "batch_labels = [f\"{b//1000}K\" if b < 1000000 else f\"{b//1000000}M\" for b in batch_sizes]\n",
    "x = np.arange(len(batch_sizes))\n",
    "width = 0.35\n",
    "\n",
    "edge_costs = [r['total_cost'] for r in results_edge]\n",
    "cloud_costs = [r['total_cost'] for r in results_cloud]\n",
    "\n",
    "# Cost comparison\n",
    "axes[0].bar(x - width/2, [max(c, 0.1) for c in edge_costs], width, label='Edge NPU', color='#38a169')\n",
    "axes[0].bar(x + width/2, cloud_costs, width, label='Cloud GPU', color='#e53e3e')\n",
    "axes[0].set_xlabel('Batch Size')\n",
    "axes[0].set_ylabel('Total Cost ($)')\n",
    "axes[0].set_title('Inference Cost: Edge NPU vs Cloud GPU')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(batch_labels)\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Latency comparison\n",
    "axes[1].bar(x - width/2, [15] * len(batch_sizes), width, label='Edge NPU', color='#38a169')\n",
    "axes[1].bar(x + width/2, [58] * len(batch_sizes), width, label='Cloud GPU', color='#e53e3e')\n",
    "axes[1].set_xlabel('Batch Size')\n",
    "axes[1].set_ylabel('Latency per Request (ms)')\n",
    "axes[1].set_title('Per-Request Latency Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(batch_labels)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('edge_vs_cloud_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š ANALYSIS:\")\n",
    "print(f\"   â€¢ Edge NPU: Zero cloud cost, {edge_npu.compute_time_ms}ms latency\")\n",
    "print(f\"   â€¢ Cloud GPU: $50/million requests, {cloud_gpu.compute_time_ms + cloud_gpu.network_latency_ms}ms latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NPU Hardware Landscape (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current NPU hardware landscape (2025 data)\n",
    "npu_hardware = [\n",
    "    {'Vendor': 'Qualcomm', 'Product': 'Snapdragon X2 Elite', 'NPU': 'Hexagon', 'TOPS': 80, 'Target': 'Laptops'},\n",
    "    {'Vendor': 'Apple', 'Product': 'M4 Chip', 'NPU': 'Neural Engine', 'TOPS': 38, 'Target': 'Mac/iPad'},\n",
    "    {'Vendor': 'Intel', 'Product': 'Panther Lake', 'NPU': 'Intel NPU 4', 'TOPS': 50, 'Target': 'Laptops'},\n",
    "    {'Vendor': 'AMD', 'Product': 'Ryzen AI Max', 'NPU': 'XDNA 2', 'TOPS': 50, 'Target': 'Workstations'},\n",
    "    {'Vendor': 'Google', 'Product': 'Tensor G4', 'NPU': 'Edge TPU', 'TOPS': 15, 'Target': 'Pixel phones'},\n",
    "    {'Vendor': 'Google', 'Product': 'Coral NPU', 'NPU': 'RISC-V', 'TOPS': 0.5, 'Target': 'IoT/Wearables'}\n",
    "]\n",
    "\n",
    "df_hardware = pd.DataFrame(npu_hardware)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NPU HARDWARE LANDSCAPE (2025)\")\n",
    "print(\"=\"*70)\n",
    "display(df_hardware)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "vendors = [f\"{h['Vendor']}\\n{h['Product']}\" for h in npu_hardware]\n",
    "tops_values = [h['TOPS'] for h in npu_hardware]\n",
    "colors = ['#4299e1', '#667eea', '#3182ce', '#e53e3e', '#38a169', '#48bb78']\n",
    "\n",
    "bars = ax.barh(vendors, tops_values, color=colors)\n",
    "ax.set_xlabel('TOPS (Trillion Operations Per Second)')\n",
    "ax.set_title('NPU Performance Comparison (2025)')\n",
    "\n",
    "for bar, val in zip(bars, tops_values):\n",
    "    ax.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val} TOPS', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('npu_tops_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "1. **NPUs are purpose-built for inference** - They optimize matrix operations at low precision\n",
    "2. **Edge inference eliminates network latency** - 0ms vs 50-200ms for cloud\n",
    "3. **Cost scales dramatically** - 1M requests: Cloud ~$50, Edge ~$0\n",
    "4. **Hardware is maturing rapidly** - 50-80 TOPS NPUs now standard\n",
    "5. **Lower precision = higher efficiency** - FP16/INT8 faster than FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 1 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ… Processor comparison analyzed\")\n",
    "print(\"âœ… Hardware detection utility created\")\n",
    "print(\"âœ… Matrix multiplication benchmarked\")\n",
    "print(\"âœ… Edge vs Cloud cost simulation complete\")\n",
    "print(\"âœ… NPU hardware landscape visualized\")\n",
    "print(\"\\nâž¡ï¸  Next: Notebook 2 - Task Routing and Model Partitioning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
